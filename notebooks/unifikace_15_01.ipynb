{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 13)\n",
      "(1000, 14)\n",
      "(0, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phone union: 1000it [00:00, 9989.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 22)\n",
      "MATCHING_PHONE_NUMBER null value count  1000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validate_phone method is running ...: 1000it [00:00, 6731.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCHING_PHONE_NUMBER null value count  868 \n",
      "\n",
      "UNIFICATION_PARTY_PHONE_ID null value count  1000\n",
      "UNIFICATION_PARTY_PHONE_ID null value count  771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Null value impute is running ...: 868it [00:00, 9210.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCHING_PHONE_NUMBER null value count  868 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from  datetime import datetime\n",
    "import phonenumbers\n",
    "from clean_phone import clean_phone\n",
    "from clean_phone import _check_phone\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def extract_columns_from_ddl(table_ddl):\n",
    "    column_names = [line.split()[0] for line in table_ddl.strip().split('\\n')]\n",
    "    return  column_names\n",
    "\n",
    "def generate_dummy_data_df(n):\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        data.append({\n",
    "            \"PARTY_PHONE_ID\": random.randint(1, 1000),\n",
    "            \"PARTY_ID\": random.randint(1, 1000),\n",
    "            \"COUNTRY_ID\": f'Country_{random.randint(1, 100)}',\n",
    "            \"PARTY_PHONE_TYPE_ID\": random.randint(1, 10),\n",
    "            \"PHONE_NUMBER\": f'{random.randint(100000, 999999)}',\n",
    "            \"PHONE_NUMBER_PREFIX\": f'+{random.randint(10, 99)}',\n",
    "            \"SRC_ID\": f'SRC_{random.randint(1, 100)}',\n",
    "            \"SRC_SYS_ID\": f'SYS_{random.randint(1, 100)}',\n",
    "            \"DEL_FLAG\": random.randint(0, 1),\n",
    "            \"INSERT_DATETIME\": datetime.now().date(),\n",
    "            \"INS_PROCESS_ID\": f'Process_{random.randint(1, 100)}',\n",
    "            \"UPDATE_DATETIME\":  pd.to_datetime('2024-01-05'),\n",
    "            \"UPD_PROCESS_ID\": f'Process_{random.randint(101, 200)}',\n",
    "            \"UPD_EFF_DATE\": datetime.now().date()\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def MTCH_PT_PHONE_definition(table_ddl):\n",
    "\n",
    "    columns = extract_columns_from_ddl(table_ddl)\n",
    "    return pd.DataFrame(columns=columns)\n",
    "\n",
    "def insert_phones(filtered_MTCH_PT_PHONE):\n",
    "    transformed_df = pd.DataFrame(\n",
    "        {\n",
    "        'PARTY_PHONE_ID': filtered_MTCH_PT_PHONE['PARTY_PHONE_ID'],\n",
    "        'PARTY_ID': filtered_MTCH_PT_PHONE['PARTY_ID'],\n",
    "        'COUNTRY_ID': filtered_MTCH_PT_PHONE['COUNTRY_ID'],\n",
    "        'PARTY_PHONE_TYPE_ID': filtered_MTCH_PT_PHONE['PARTY_PHONE_TYPE_ID'],\n",
    "        'PHONE_NUMBER': filtered_MTCH_PT_PHONE['PHONE_NUMBER'],\n",
    "        'PHONE_NUMBER_PREFIX': filtered_MTCH_PT_PHONE['PHONE_NUMBER_PREFIX'],\n",
    "        'SRC_ID': filtered_MTCH_PT_PHONE['SRC_ID'],\n",
    "        'SRC_SYS_ID': filtered_MTCH_PT_PHONE['SRC_SYS_ID'],\n",
    "        'DELETE_FLAG': filtered_MTCH_PT_PHONE['DEL_FLAG'],\n",
    "        'INSERT_DATETIME': filtered_MTCH_PT_PHONE['INSERT_DATETIME'],\n",
    "        'INSERT_PROCESS_ID': filtered_MTCH_PT_PHONE['INS_PROCESS_ID'],\n",
    "        'UPDATE_DATETIME': filtered_MTCH_PT_PHONE['UPDATE_DATETIME'],\n",
    "        'UPDATE_DATETIME_PROCESS_ID': filtered_MTCH_PT_PHONE['UPD_PROCESS_ID'],\n",
    "        'UPDATE_DATETIME_EFFECTIVE_DATE': filtered_MTCH_PT_PHONE['UPD_EFF_DATE']\n",
    "        }\n",
    "        \n",
    "    )\n",
    "    return transformed_df\n",
    "\n",
    "def validate_phone_number(number):\n",
    "    try:\n",
    "        parsed_number = phonenumbers.parse(str(number), None)\n",
    "        return phonenumbers.is_valid_number(parsed_number)\n",
    "    except phonenumbers.NumberParseException:\n",
    "        return False\n",
    "\n",
    "def unified_phone_id(df,column_for_unification,group_by_column, phone_key, result_column):\n",
    "    \"\"\"\n",
    "        df :  pd.DataFrame() with party phone numbers\n",
    "        column_for_unification :  column which contains valid phone numbers for unification\n",
    "        group_by_column : primary key, based on which the data will be grouped \n",
    "        phone_key : primary key, which identifies the phone\n",
    "        result_column : name of the result column, where the result values will be added\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_df = df[df[column_for_unification].notnull() & ~df[column_for_unification].isna()]\n",
    "    grouped_phones = filtered_df.groupby(group_by_column)[phone_key].apply(list)\n",
    "    df[result_column] = df[group_by_column].map(grouped_phones)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_null_phone_numbers(df, column, value):\n",
    "    for index, row in tqdm(df[df[column].isnull()].iterrows(),desc = \"Null value impute is running ...\"):\n",
    "        df.loc[index,column] = value\n",
    "    return df \n",
    "\n",
    "\n",
    "def list_to_string_transformation(df: pd.DataFrame, column: str):\n",
    "    for index, row in tqdm(df.iterrows(), desc=\"List transformation is running ...\"):\n",
    "        # Check if the entry is iterable (e.g., a list), if not, skip the iteration.\n",
    "        if not isinstance(row[column], list):\n",
    "            #print(f\"Non-iterable item at index {index}: {row['UNIFICATION_PARTY_PHONE_ID']}\")\n",
    "            continue\n",
    "        \n",
    "        resulted_string = \"\"\n",
    "        for val in row[column]:\n",
    "            # First method\n",
    "            values_as_string = [\"'{}'\".format(int(val)) for val in row[column]]\n",
    "            resulted_string = ', '.join(values_as_string)\n",
    "            df.loc[index, column]\n",
    "            \n",
    "            # Properly concatenate the string with each value.\n",
    "        df.loc[index, column] = resulted_string\n",
    "    return df\n",
    "\n",
    "ddl = \"\"\"\n",
    "    SORURCE_PHONE_PREFIX VARCHAR(255),\n",
    "    SORURCE_PHONE_NUMBER VARCHAR(255),\n",
    "    MATCHING_PHONE_NUMBER VARCHAR(255),\n",
    "    MASTER_PARTY_PHONE_ID INTEGER,\n",
    "    UNIFICATION_PARTY_PHONE_ID INTEGER,\n",
    "    SOURCE_IDENTIFIER VARCHAR(255),\n",
    "    SOURCE_SYSTEM_IDENTIFIER VARCHAR(255),\n",
    "    DELETE_FLAG INTEGER,\n",
    "    INSERT_DATETIME DATE,\n",
    "    INSERT_PROCESS_ID VARCHAR(255),\n",
    "    UPDATE_DATETIME DATE,\n",
    "    UPDATE_DATETIME_PROCESS_ID VARCHAR(255),\n",
    "    UPDATE_DATETIME_EFFECTIVE_DATE DATE\n",
    "\"\"\"\n",
    "MTCH_PT_PHONE = MTCH_PT_PHONE_definition(ddl)\n",
    "PARTY_PHONE = generate_dummy_data_df(1000)\n",
    "PARTY_PHONE = PARTY_PHONE.drop_duplicates()\n",
    "\n",
    "#Type correction\n",
    "MTCH_PT_PHONE['UPDATE_DATETIME'] = pd.to_datetime(MTCH_PT_PHONE['UPDATE_DATETIME'], errors='coerce')\n",
    "MTCH_PT_PHONE['INSERT_DATETIME'] = pd.to_datetime(MTCH_PT_PHONE['INSERT_DATETIME'], errors='coerce')\n",
    "\n",
    "print(MTCH_PT_PHONE.shape)\n",
    "print(PARTY_PHONE.shape)\n",
    "\n",
    "\n",
    "print(MTCH_PT_PHONE.shape)\n",
    "last_checked_date = pd.to_datetime('2024-01-10')\n",
    "filtered_MTCH_PT = PARTY_PHONE[PARTY_PHONE['UPDATE_DATETIME']<last_checked_date]\n",
    "MTCH_PT_PHONE = pd.concat([MTCH_PT_PHONE, insert_phones(filtered_MTCH_PT)], ignore_index=True)\n",
    "for index, row in tqdm(MTCH_PT_PHONE.iterrows(),desc = \"Phone union\"):\n",
    "     MTCH_PT_PHONE.loc[index,'MATCHING_PHONE_NUMBER_TMP'] = row['PHONE_NUMBER_PREFIX'] + row['PHONE_NUMBER']\n",
    "print(MTCH_PT_PHONE.shape)\n",
    "\n",
    "from clean_phone import validate_phone\n",
    "print(\"MATCHING_PHONE_NUMBER null value count \",MTCH_PT_PHONE.MATCHING_PHONE_NUMBER.isna().sum(),\"\\n\")\n",
    "\n",
    "#res = []\n",
    "for index, row in tqdm(MTCH_PT_PHONE.iterrows(), \"validate_phone method is running ...\"):\n",
    "    #res.append(_check_phone(row['MATCHING_PHONE_NUMBER_TMP'],False))\n",
    "    phone_number_str = row['PHONE_NUMBER_PREFIX'] + row['PHONE_NUMBER']\n",
    "    if validate_phone(phone_number_str):\n",
    "        MTCH_PT_PHONE.loc[index, 'MATCHING_PHONE_NUMBER'] = phonenumbers.format_number(phonenumbers.parse(phone_number_str, None), phonenumbers.PhoneNumberFormat.INTERNATIONAL)   \n",
    "    else:\n",
    "        MTCH_PT_PHONE.loc[index, 'MATCHING_PHONE_NUMBER'] = None \n",
    "\n",
    "print(\"MATCHING_PHONE_NUMBER null value count \",MTCH_PT_PHONE.MATCHING_PHONE_NUMBER.isna().sum(),\"\\n\")\n",
    "\n",
    "print(\"UNIFICATION_PARTY_PHONE_ID null value count \",MTCH_PT_PHONE['UNIFICATION_PARTY_PHONE_ID'].isna().sum())\n",
    "\n",
    "MTCH_PT_PHONE = unified_phone_id(df = MTCH_PT_PHONE,\n",
    "                                column_for_unification = 'MATCHING_PHONE_NUMBER',\n",
    "                                group_by_column = 'PARTY_ID',\n",
    "                                phone_key = 'PARTY_PHONE_ID',\n",
    "                                result_column =  'UNIFICATION_PARTY_PHONE_ID')\n",
    "\n",
    "print(\"UNIFICATION_PARTY_PHONE_ID null value count \",MTCH_PT_PHONE['UNIFICATION_PARTY_PHONE_ID'].isna().sum())\n",
    "\n",
    "MTCH_PT_PHONE = impute_null_phone_numbers(MTCH_PT_PHONE,'MATCHING_PHONE_NUMBER',None)\n",
    "print(\"MATCHING_PHONE_NUMBER null value count \",MTCH_PT_PHONE.MATCHING_PHONE_NUMBER.isna().sum(),\"\\n\")\n",
    "\n",
    "MTCH_PT = pd.read_csv('mtch_pt.csv')\n",
    "MTCH_PT['UNIFICATION_PARTY_PHONE_ID'] = None\n",
    "#MTCH_PT = MTCH_PT.rename(columns={\"MATCHING_MFO\": \"UNIFICATION_PARTY_PHONE_ID\"})\n",
    "#merged_df = pd.merge(MTCH_PT, MTCH_PT_PHONE[['PARTY_ID', 'UNIFICATION_PARTY_PHONE_ID']], on='PARTY_ID', how='left')\n",
    "\n",
    "merged_df = pd.merge(MTCH_PT, \n",
    "                     MTCH_PT_PHONE[['PARTY_ID', 'UNIFICATION_PARTY_PHONE_ID']], \n",
    "                     on='PARTY_ID', \n",
    "                     how='left',\n",
    "                     suffixes=('', '_from_phone'))\n",
    "\n",
    "# Update UNIFICATION_PARTY_PHONE_ID in MTCH_PT with the values from MTCH_PT_PHONE\n",
    "MTCH_PT['UNIFICATION_PARTY_PHONE_ID'] = merged_df['UNIFICATION_PARTY_PHONE_ID_from_phone']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emails part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12)\n",
      "(0, 12)\n",
      "(0, 12)\n",
      "(1000, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EMAIL union: 1000it [00:00, 11079.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 17)\n",
      "MATCHING_EMAIL null value count  1000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Email validation method is running ...: 1000it [00:00, 5722.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MATCHING_EMAIL null value count  0\n",
      "\n",
      "\n",
      "UNIFICATION_PARTY_EMAIL_ID null value count  1000\n",
      "\n",
      "\n",
      "UNIFICATION_PARTY_EMAIL_ID null value count  0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Null value impute is running ...: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCHING_EMAIL null value count  0\n"
     ]
    }
   ],
   "source": [
    "from pyisemail import is_email\n",
    "\n",
    "email_df = pd.read_csv(\"generated-emails-32000.csv\", header=0, names=['email'])\n",
    "\n",
    "def extract_columns_from_ddl(table_ddl):\n",
    "    column_names = [line.split()[0] for line in table_ddl.strip().split('\\n')]\n",
    "    return  column_names\n",
    "\n",
    "def generate_dummy_data_email_df(n):\n",
    "    data = []\n",
    "    max_index = email_df.shape[0]\n",
    "    i = 0\n",
    "    for _ in range(n):\n",
    "        if i == max_index - 2:\n",
    "            i = 0\n",
    "        data.append({\n",
    "            \"PARTY_EMAIL_ID\": random.randint(1, 1000),\n",
    "            \"PARTY_ID\": random.randint(1, 1000),\n",
    "            #\"COUNTRY_ID\": f'Country_{random.randint(1, 100)}',\n",
    "            \"PARTY_EMAIL_TYPE_ID\": random.randint(1, 10),\n",
    "            \"EMAIL\": email_df[\"email\"][i],\n",
    "            #\"EMAIL_NUMBER_PREFIX\": f'+{random.randint(10, 99)}',\n",
    "            \"SRC_ID\": f'SRC_{random.randint(1, 100)}',\n",
    "            \"SRC_SYS_ID\": f'SYS_{random.randint(1, 100)}',\n",
    "            \"DEL_FLAG\": random.randint(0, 1),\n",
    "            \"INSERT_DATETIME\": datetime.now().date(),\n",
    "            \"INS_PROCESS_ID\": f'Process_{random.randint(1, 100)}',\n",
    "            \"UPDATE_DATETIME\":  pd.to_datetime('2024-01-05'),\n",
    "            \"UPD_PROCESS_ID\": f'Process_{random.randint(101, 200)}',\n",
    "            \"UPD_EFF_DATE\": datetime.now().date()\n",
    "        })\n",
    "        i += 1\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def MTCH_PT_EMAIL_definition(table_ddl):\n",
    "\n",
    "    columns = extract_columns_from_ddl(table_ddl)\n",
    "    return pd.DataFrame(columns=columns)\n",
    "\n",
    "def insert_emails(df : pd.DataFrame):\n",
    "    transformed_df = pd.DataFrame(\n",
    "        {\n",
    "        'PARTY_EMAIL_ID': df['PARTY_EMAIL_ID'],\n",
    "        'PARTY_ID': df['PARTY_ID'],\n",
    "        #'COUNTRY_ID': df['COUNTRY_ID'],\n",
    "        'PARTY_EMAIL_TYPE_ID': df['PARTY_EMAIL_TYPE_ID'],\n",
    "        'SOURCE_EMAIL': df['EMAIL'],\n",
    "        \n",
    "        'SRC_ID': df['SRC_ID'],\n",
    "        'SRC_SYS_ID': df['SRC_SYS_ID'],\n",
    "        'DELETE_FLAG': df['DEL_FLAG'],\n",
    "        'INSERT_DATETIME': df['INSERT_DATETIME'],\n",
    "        'INSERT_PROCESS_ID': df['INS_PROCESS_ID'],\n",
    "        'UPDATE_DATETIME': df['UPDATE_DATETIME'],\n",
    "        'UPDATE_DATETIME_PROCESS_ID': df['UPD_PROCESS_ID'],\n",
    "        'UPDATE_DATETIME_EFFECTIVE_DATE': df['UPD_EFF_DATE']\n",
    "        }\n",
    "        \n",
    "    )\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "def impute_null_values(df, column, value):\n",
    "    for index, row in tqdm(df[df[column].isnull()].iterrows(),desc = \"Null value impute is running ...\"):\n",
    "        df.loc[index,column] = value\n",
    "    return df \n",
    "\n",
    "def unify_email(df,column_for_unification,group_by_column, phone_key, result_column):\n",
    "    \"\"\"\n",
    "        df :  pd.DataFrame() with party email data\n",
    "        column_for_unification :  column which contains valid email for unification\n",
    "        group_by_column :  key, based on which the data will be grouped \n",
    "        phone_key : primary key, which identifies the email\n",
    "        result_column : name of the result column, where the result values will be added\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_df = df[df[column_for_unification].notnull() & ~df[column_for_unification].isna()]\n",
    "    grouped_phones = filtered_df.groupby(group_by_column)[phone_key].apply(list)\n",
    "    df[result_column] = df[group_by_column].map(grouped_phones)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def list_to_string_transformation(df: pd.DataFrame, column: str):\n",
    "    for index, row in tqdm(df.iterrows(), desc=\"List transformation is running ...\"):\n",
    "        # Check if the entry is iterable (e.g., a list), if not, skip the iteration.\n",
    "        if not isinstance(row[column], list):\n",
    "            #print(f\"Non-iterable item at index {index}: {row['UNIFICATION_PARTY_PHONE_ID']}\")\n",
    "            continue\n",
    "        \n",
    "        resulted_string = \"\"\n",
    "        for val in row[column]:\n",
    "            # First method\n",
    "            values_as_string = [\"'{}'\".format(int(val)) for val in row[column]]\n",
    "            resulted_string = ', '.join(values_as_string)\n",
    "            df.loc[index, column]\n",
    "            \n",
    "            # Properly concatenate the string with each value.\n",
    "        df.loc[index, column] = resulted_string\n",
    "    return df\n",
    "\n",
    "ddl = \"\"\"\n",
    "    SOURCE_EMAIL VARCHAR(255),\n",
    "    MATCHING_EMAIL VARCHAR(255),\n",
    "    MASTER_PARTY_EMAIL_ID INTEGER,\n",
    "    UNIFICATION_PARTY_EMAIL_ID INTEGER,\n",
    "    SOURCE_IDENTIFIER VARCHAR(255),\n",
    "    SOURCE_SYSTEM_IDENTIFIER VARCHAR(255),\n",
    "    DELETE_FLAG INTEGER,\n",
    "    INSERT_DATETIME DATE,\n",
    "    INSERT_PROCESS_ID VARCHAR(255),\n",
    "    UPDATE_DATETIME DATE,\n",
    "    UPDATE_DATETIME_PROCESS_ID VARCHAR(255),\n",
    "    UPDATE_DATETIME_EFFECTIVE_DATE DATE\n",
    "\"\"\"\n",
    "MTCH_PT_EMAIL = MTCH_PT_EMAIL_definition(ddl)\n",
    "PARTY_EMAIL = generate_dummy_data_email_df(1000)\n",
    "PARTY_EMAIL = PARTY_EMAIL.drop_duplicates()\n",
    "\n",
    "#Type correction\n",
    "MTCH_PT_EMAIL['UPDATE_DATETIME'] = pd.to_datetime(MTCH_PT_EMAIL['UPDATE_DATETIME'], errors='coerce')\n",
    "MTCH_PT_EMAIL['INSERT_DATETIME'] = pd.to_datetime(MTCH_PT_EMAIL['INSERT_DATETIME'], errors='coerce')\n",
    "\n",
    "print(PARTY_EMAIL.shape)\n",
    "print(MTCH_PT_EMAIL.shape)\n",
    "\n",
    "## MTCH table population\n",
    " #- Insert not validated data \n",
    "\n",
    "print(MTCH_PT_EMAIL.shape)\n",
    "last_checked_date = pd.to_datetime('2024-01-10')\n",
    "filtered_MTCH_PT = PARTY_EMAIL[PARTY_EMAIL['UPDATE_DATETIME']<last_checked_date]\n",
    "print(filtered_MTCH_PT.shape)\n",
    "MTCH_PT_EMAIL = pd.concat([MTCH_PT_EMAIL, insert_emails(filtered_MTCH_PT)], ignore_index=True)\n",
    "for index, row in tqdm(MTCH_PT_EMAIL.iterrows(),desc = \"EMAIL union\"):\n",
    "    MTCH_PT_EMAIL.loc[index,'SOURCE_EMAIL'] = row['SOURCE_EMAIL'].lower()\n",
    "print(MTCH_PT_EMAIL.shape)\n",
    "\n",
    "\n",
    "print(\"MATCHING_EMAIL null value count \",MTCH_PT_EMAIL.MATCHING_EMAIL.isna().sum(),\"\\n\")\n",
    "\n",
    "for index, row in tqdm(MTCH_PT_EMAIL.iterrows(), \"Email validation method is running ...\",mininterval=1):\n",
    "    # Assuming 'EMAIL_NUMBER' is the actual phone number column\n",
    "    if is_email(row['SOURCE_EMAIL'], check_dns=False):       \n",
    "        MTCH_PT_EMAIL.loc[index, 'MATCHING_EMAIL'] = row['SOURCE_EMAIL']\n",
    "    else:\n",
    "        MTCH_PT_EMAIL.loc[index, 'MATCHING_EMAIL'] = None\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"MATCHING_EMAIL null value count \",MTCH_PT_EMAIL.MATCHING_EMAIL.isna().sum())\n",
    "print(\"\\n\")\n",
    "print(\"UNIFICATION_PARTY_EMAIL_ID null value count \",MTCH_PT_EMAIL['UNIFICATION_PARTY_EMAIL_ID'].isna().sum())\n",
    "print(\"\\n\")\n",
    "MTCH_PT_EMAIL = unify_email(MTCH_PT_EMAIL,'MATCHING_EMAIL','PARTY_ID', 'PARTY_EMAIL_ID', 'UNIFICATION_PARTY_EMAIL_ID')\n",
    "print(\"UNIFICATION_PARTY_EMAIL_ID null value count \",MTCH_PT_EMAIL['UNIFICATION_PARTY_EMAIL_ID'].isna().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "MTCH_PT_EMAIL = impute_null_values(MTCH_PT_EMAIL,'MATCHING_EMAIL',None)\n",
    "print(\"MATCHING_EMAIL null value count \",MTCH_PT_EMAIL.MATCHING_EMAIL.isna().sum())\n",
    "\n",
    "MTCH_PT['UNIFICATION_PARTY_EMAIL_ID'] = None\n",
    "#MTCH_PT = MTCH_PT.rename(columns={\"MATCHING_MFO\": \"UNIFICATION_PARTY_PHONE_ID\"})\n",
    "#merged_df = pd.merge(MTCH_PT, MTCH_PT_EMAIL[['PARTY_ID', 'UNIFICATION_PARTY_PHONE_ID']], on='PARTY_ID', how='left')\n",
    "\n",
    "merged_df = pd.merge(MTCH_PT, \n",
    "                     MTCH_PT_EMAIL[['PARTY_ID', 'UNIFICATION_PARTY_EMAIL_ID']], \n",
    "                     on='PARTY_ID', \n",
    "                     how='left',\n",
    "                     suffixes=('', '_from_email'))\n",
    "\n",
    "# Update UNIFICATION_PARTY_PHONE_ID in MTCH_PT with the values from MTCH_PT_EMAIL\n",
    "MTCH_PT['UNIFICATION_PARTY_EMAIL_ID'] = merged_df['UNIFICATION_PARTY_EMAIL_ID_from_email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 33)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCTH_TMP_PT = pd.read_csv('mcth_tmp_pt.csv')\n",
    "MCTH_TMP_PT.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTH_TMP_PT shape:  (0, 33)\n",
      "MTCH_PT shape:  (32029, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rd/l77bltt96_97jdz4bbkmq5z40000gn/T/ipykernel_25217/2667643123.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample.UPDATE_DATETIME = '2023-12-01'\n",
      "/var/folders/rd/l77bltt96_97jdz4bbkmq5z40000gn/T/ipykernel_25217/2667643123.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_PO.MATCHING_PARTY_TYPE_ID = 'PO'\n",
      "/var/folders/rd/l77bltt96_97jdz4bbkmq5z40000gn/T/ipykernel_25217/2667643123.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_PO.SOURCE_PARTY_TYPE_ID = 'PO'\n",
      "/var/folders/rd/l77bltt96_97jdz4bbkmq5z40000gn/T/ipykernel_25217/2667643123.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_FOP.MATCHING_PARTY_TYPE_ID = 'FOP'\n",
      "/var/folders/rd/l77bltt96_97jdz4bbkmq5z40000gn/T/ipykernel_25217/2667643123.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_FOP.SOURCE_PARTY_TYPE_ID = 'FOP'\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Filter out the specific deprecation warning\n",
    "warnings.filterwarnings('ignore', message='The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.')\n",
    "warnings.filterwarnings('ignore', message='SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead')\n",
    "\n",
    "np.random.seed(123)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def insertion(df):\n",
    "    transformed_df = pd.DataFrame({\n",
    "    'MATCHING_FAMILY_NAME_LATIN': df['SOURCE_FAMILY_NAME_LATIN'],\n",
    "    'MATCHING_BIRTH_DATE': df['SOURCE_BIRTH_DATE'],\n",
    "    'SOURCE_SYSTEM_IDENTIFIER': df['SOURCE_SYSTEM_IDENTIFIER'],\n",
    "    'SOURCE_SYSTEM': None,  \n",
    "    'PREVIOUS_CONSOLIDATED_PARTY_ID': df['PREVIOUS_CONSOLIDATED_PARTY_ID'],\n",
    "    'UNIFICATION_PARTY_PHONE_ID': df['UNIFICATION_PARTY_PHONE_ID'], #TODO:\n",
    "    'UNIFICATION_PARTY_EMAIL_ID': df['UNIFICATION_PARTY_EMAIL_ID'], #TODO:\n",
    "    #'MATCHING_MFO_ID': None,  TODO: Delete \n",
    "    'MATCHING_PARTY_ID': df['PARTY_ID'], \n",
    "    'L1_MASTER_MATCHING_RULE': df['L1_MASTER_MATCHING_RULE'],\n",
    "    'MATCHING_EMPLOYER_IDENTIFIER': None,  # TODO: Assuming NULL\n",
    "    'INSERT_PROCESS_ID': df['INSERT_PROCESS_ID'],\n",
    "    'MATCHING_UNIFIED_PARTY_ID': df['MATCHING_UNIFIED_PARTY_ID'],\n",
    "    'MATCHING_COMPANY_ID': df['MATCHING_COMPANY_IDENTIFIER'],  \n",
    "    'PREVIOUS_UNIFIED_PARTY_ID': df['PREVIOUS_UNIFIED_PARTY_ID'],\n",
    "    'L1_RECORD_ROLE': df['L1_RECORD_ROLE'],\n",
    "    'PARTY_ID': df['PARTY_ID'],\n",
    "    'L2_MASTER_PARTY_ID': df['L2_MASTER_PARTY_ID'],\n",
    "    'UPDATE_EFFECTIVE_DATE': None,  \n",
    "    'MATCHING_FIRST_NAME': df['MATCHING_FIRST_NAME'],\n",
    "    'INSERT_DATETIME': df['INSERT_DATETIME'],\n",
    "    'WF_JOB_ID': None, \n",
    "    'MATCHING_COUNTRY_ID': df['MATCHING_COUNTRY_ID'],\n",
    "    'UPDATE_DATETIME': df['UPDATE_DATETIME'],\n",
    "    'L2_RECORD_ROLE': df['L2_RECORD_ROLE'],\n",
    "    'MATCHING_PARTY_IDENTIFIER': df['SOURCE_PERSON_IDENTIFIER'], \n",
    "    'MATCHING_PARTY_TYPE_ID': df['MATCHING_PARTY_TYPE_ID'],\n",
    "    'MATCHING_PERSON_TYPE_ID': df['MATCHING_PARTY_TYPE_ID'],\n",
    "    'DELETE_FLAG': None,\n",
    "    'MATCHING_CONSOLIDATED_PARTY_ID': df['MATCHING_CONSOLIDATED_PARTY_ID'],\n",
    "    'L2_RECORD_MATCHING_RULE': df['L2_RECORD_MATCHING_RULE'],\n",
    "    'MATCHING_FIRST_NAME_LATIN': df['MATCHING_FIRST_NAME_LATIN'],\n",
    "    'UPDATE_PROCESS_ID': None,\n",
    "    #'MATCHING_CI_CODE': None,  TODO: Delete \n",
    "    'MATCHING_FAMILY_NAME': df['MATCHING_FAMILY_NAME'],\n",
    "    'L1_MASTER_PARTY_ID': df['L1_MASTER_PARTY_ID']\n",
    "})\n",
    "    return transformed_df\n",
    "\n",
    "def insert_all(MTCH_PT,MCTH_TMP_PT):\n",
    "    \"\"\" Inserts all rows to tmp table to run initial unification \"\"\"\n",
    "    print(\"###   Running insert_all   ###\")\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, insertion(MTCH_PT)], ignore_index=True)\n",
    "        \n",
    "    return MCTH_TMP_PT\n",
    "\n",
    "\n",
    "print(\"MCTH_TMP_PT shape: \", MCTH_TMP_PT.shape)\n",
    "print(\"MTCH_PT shape: \", MTCH_PT.shape)\n",
    "\n",
    "# birth date correction \n",
    "MTCH_PT['SOURCE_BIRTH_DATE'] = MTCH_PT['SOURCE_BIRTH_DATE'].str.replace('/', '.')\n",
    "MTCH_PT['MATCHING_BIRTH_DATE'] = pd.to_datetime(MTCH_PT['SOURCE_BIRTH_DATE']).dt.date #, format='%m.%d.%Y')\n",
    "\n",
    "\n",
    "sample_id = MTCH_PT['PARTY_ID'].head(100).tolist() + MTCH_PT['PARTY_ID'].tail(100).tolist() +  MTCH_PT['PARTY_ID'].iloc[10200:10400].tolist()\n",
    "\n",
    "samp_PO = MTCH_PT['PARTY_ID'].iloc[0:10676].tolist()\n",
    "samp_FOP = MTCH_PT['PARTY_ID'].iloc[21352:MTCH_PT.shape[0]].tolist()\n",
    "\n",
    "sample = MTCH_PT.loc[MTCH_PT['PARTY_ID'].isin(sample_id)]\n",
    "sample_PO = MTCH_PT.loc[MTCH_PT['PARTY_ID'].isin(samp_PO)]\n",
    "sample_FOP = MTCH_PT.loc[MTCH_PT['PARTY_ID'].isin(samp_FOP)]\n",
    "\n",
    "sample.UPDATE_DATETIME = '2023-12-01'\n",
    "sample_PO.MATCHING_PARTY_TYPE_ID = 'PO'\n",
    "sample_PO.SOURCE_PARTY_TYPE_ID = 'PO'\n",
    "sample_FOP.MATCHING_PARTY_TYPE_ID = 'FOP'\n",
    "sample_FOP.SOURCE_PARTY_TYPE_ID = 'FOP'\n",
    "\n",
    "MTCH_PT.set_index('PARTY_ID', inplace=True)\n",
    "sample.set_index('PARTY_ID', inplace=True)\n",
    "sample_PO.set_index('PARTY_ID', inplace=True)\n",
    "sample_FOP.set_index('PARTY_ID', inplace=True)\n",
    "\n",
    "# Update the MTCH_PT DataFrame with the modified sample DataFrame\n",
    "MTCH_PT.update(sample)\n",
    "MTCH_PT.update(sample_PO)\n",
    "MTCH_PT.update(sample_FOP)\n",
    "\n",
    "# Reset the index if needed\n",
    "MTCH_PT.reset_index(inplace=True)\n",
    "# Data Normalisation\n",
    "MCTH_TMP_PT['MATCHING_FAMILY_NAME'] = MCTH_TMP_PT['MATCHING_FAMILY_NAME'].str.lower().str.strip()\n",
    "#MCTH_TMP_PT['MATCHING_FAMILY_NAME_LATIN'] = MCTH_TMP_PT['MATCHING_FAMILY_NAME_LATIN'].str.lower().str.strip()\n",
    "MCTH_TMP_PT['MATCHING_FIRST_NAME'] = MCTH_TMP_PT['MATCHING_FIRST_NAME'].str.lower().str.strip()\n",
    "#MCTH_TMP_PT['MATCHING_FIRST_NAME_LATIN'] = MCTH_TMP_PT['MATCHING_FIRST_NAME_LATIN'].str.lower().str.strip()\n",
    "MCTH_TMP_PT['MATCHING_COUNTRY_ID'] = MCTH_TMP_PT['MATCHING_COUNTRY_ID'].str.lower().str.strip()\n",
    "# # Dates\n",
    "MCTH_TMP_PT['MATCHING_BIRTH_DATE'] = pd.to_datetime(MCTH_TMP_PT['MATCHING_BIRTH_DATE'], errors='coerce')\n",
    "\n",
    "MTCH_PT['UPDATE_DATETIME'] = pd.to_datetime(MTCH_PT['UPDATE_DATETIME'], errors='coerce')\n",
    "#Drop Duplicates\n",
    "MCTH_TMP_PT = MCTH_TMP_PT.drop_duplicates()\n",
    "last_checked_date = pd.to_datetime('2023-12-05')\n",
    "#filtered_MTCH_PT = MTCH_PT # Unifikace na vsech datech\n",
    "filtered_MTCH_PT = MTCH_PT[MTCH_PT['UPDATE_DATETIME']<last_checked_date] # & PK != PK_TMP\n",
    "transformed_df = insertion(filtered_MTCH_PT)\n",
    "# Append the transformed data to the MCTH_TMP_PT DataFrame\n",
    "MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE_PARTY_TYPE_ID\n",
      "FOP    10677\n",
      "PO     10676\n",
      "FO     10676\n",
      "Name: count, dtype: int64\n",
      "(400, 37)\n",
      "(32029, 46)\n",
      "MATCHING_PERSON_TYPE_ID\n",
      "PO     300\n",
      "FOP    100\n",
      "Name: count, dtype: int64\n",
      "MATCHING_PARTY_TYPE_ID\n",
      "PO     300\n",
      "FOP    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(MTCH_PT['SOURCE_PARTY_TYPE_ID'].value_counts())\n",
    "\n",
    "print(MCTH_TMP_PT.shape)\n",
    "print(MTCH_PT.shape)\n",
    "\n",
    "print(MCTH_TMP_PT['MATCHING_PERSON_TYPE_ID'].value_counts())\n",
    "print(MCTH_TMP_PT['MATCHING_PARTY_TYPE_ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Shape after comparing PERSONAL_IDENTIFIER  400  - 0 Rows Were Added\n",
      "Shape after comparing FAMILY_NAME  400  - 0 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_PHONE_ID  617  - 217 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_EMAIL_ID  1389  - 772 Rows Were Added\n",
      "Shape after comparing FIRST_NAME  10603  - 9214 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  10606  - 3 Rows Were Added\n",
      "Shape after comparing UNIFIED_PARTY_ID  10606  - 0 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  10606  - 0 Rows Were Added\n",
      "############\n",
      "\n",
      "Number of rows added after a cycle 10206\n",
      "2\n",
      "Shape after comparing PERSONAL_IDENTIFIER  10606  - 0 Rows Were Added\n",
      "Shape after comparing FAMILY_NAME  10703  - 97 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_PHONE_ID  10703  - 0 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_EMAIL_ID  10703  - 0 Rows Were Added\n",
      "Shape after comparing FIRST_NAME  11263  - 560 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  11263  - 0 Rows Were Added\n",
      "Shape after comparing UNIFIED_PARTY_ID  11263  - 0 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  11263  - 0 Rows Were Added\n",
      "############\n",
      "\n",
      "Number of rows added after a cycle 657\n",
      "3\n",
      "Shape after comparing PERSONAL_IDENTIFIER  11263  - 0 Rows Were Added\n",
      "Shape after comparing FAMILY_NAME  11275  - 12 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_PHONE_ID  11275  - 0 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_EMAIL_ID  11275  - 0 Rows Were Added\n",
      "Shape after comparing FIRST_NAME  11347  - 72 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  11347  - 0 Rows Were Added\n",
      "Shape after comparing UNIFIED_PARTY_ID  11347  - 0 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  11347  - 0 Rows Were Added\n",
      "############\n",
      "\n",
      "Number of rows added after a cycle 84\n",
      "4\n",
      "Shape after comparing PERSONAL_IDENTIFIER  11347  - 0 Rows Were Added\n",
      "Shape after comparing FAMILY_NAME  11347  - 0 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_PHONE_ID  11347  - 0 Rows Were Added\n",
      "Shape after comparing UNIFICATION_PARTY_EMAIL_ID  11347  - 0 Rows Were Added\n",
      "Shape after comparing FIRST_NAME  11347  - 0 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  11347  - 0 Rows Were Added\n",
      "Shape after comparing UNIFIED_PARTY_ID  11347  - 0 Rows Were Added\n",
      "Shape after comparing CONSOLIDATED_PARTY_ID  11347  - 0 Rows Were Added\n",
      "############\n",
      "\n",
      "Number of rows added after a cycle 0\n"
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "num_rows = 1 \n",
    "i = 1\n",
    "#with tqdm(total=100) as pbar:\n",
    "while num_rows>0:\n",
    "    print(i)\n",
    "    before_adding = last_count = MCTH_TMP_PT.shape[0]\n",
    "    #Match by primary identification\n",
    "    filtered_MTCH_PT = MTCH_PT[(MTCH_PT['SOURCE_PERSON_IDENTIFIER'].isin(MCTH_TMP_PT['MATCHING_PARTY_IDENTIFIER']) |\n",
    "                               (~MTCH_PT['MATCHING_COMPANY_IDENTIFIER'].isna() & MTCH_PT['MATCHING_COMPANY_IDENTIFIER'].isin(MCTH_TMP_PT['MATCHING_COMPANY_ID']))) &\n",
    "                               ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID']) ] # & PK != PK_TMP\n",
    "    transformed_df = insertion(filtered_MTCH_PT)\n",
    "    \n",
    "    # Append the transformed data to the MCTH_TMP_PT DataFrame\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "    print(\"Shape after comparing PERSONAL_IDENTIFIER \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "          \n",
    "    #########\n",
    "          \n",
    "    filtered_MTCH_PT = MTCH_PT[MTCH_PT['MATCHING_FAMILY_NAME'].isin(MCTH_TMP_PT['MATCHING_FAMILY_NAME'])&\n",
    "                               ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID'])]\n",
    "    transformed_df = insertion(filtered_MTCH_PT)\n",
    "\n",
    "    # Append the transformed data to the MCTH_TMP_PT DataFrame\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "\n",
    "    print(\"Shape after comparing FAMILY_NAME \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "\n",
    "    # Phones :\n",
    "    # phone_list = []\n",
    "    # for index, row in MCTH_TMP_PT.iterrows():\n",
    "    #     for val in row['UNIFICATION_PARTY_PHONE_ID']:\n",
    "    #         phone_list.append(val)\n",
    "    # phone_set = set(phone_list)\n",
    "    \n",
    "    # filtered_MTCH_PT = MTCH_PT[MTCH_PT['MATCHING_FAMILY_NAME'].isin(MCTH_TMP_PT['MATCHING_FAMILY_NAME'])&\n",
    "    #                            ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID'])]\n",
    "\n",
    "    phone_set = set()\n",
    "    for ids in MTCH_PT['UNIFICATION_PARTY_PHONE_ID']:\n",
    "        if isinstance(ids, (list, set, tuple)):\n",
    "            phone_set.update(ids)\n",
    "\n",
    "    filtered_MTCH_PT = MTCH_PT[\n",
    "    MTCH_PT['UNIFICATION_PARTY_PHONE_ID'].apply(lambda ids: isinstance(ids, (list, set, tuple)) and any(id in phone_set for id in ids)) &\n",
    "    ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID'])]     \n",
    "\n",
    "    transformed_df = insertion(filtered_MTCH_PT)  \n",
    "\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "    print(\"Shape after comparing UNIFICATION_PARTY_PHONE_ID \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "\n",
    "    \n",
    "\n",
    "    email_set = set()\n",
    "    for ids in MTCH_PT['UNIFICATION_PARTY_EMAIL_ID']:\n",
    "        if isinstance(ids, (list, set, tuple)):\n",
    "            email_set.update(ids)\n",
    "\n",
    "    filtered_MTCH_PT = MTCH_PT[\n",
    "    MTCH_PT['UNIFICATION_PARTY_EMAIL_ID'].apply(lambda ids: isinstance(ids, (list, set, tuple)) and any(id in email_set for id in ids)) &\n",
    "    ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID'])]     \n",
    "\n",
    "    transformed_df = insertion(filtered_MTCH_PT)  \n",
    "\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "    print(\"Shape after comparing UNIFICATION_PARTY_EMAIL_ID \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "    # TODO: COMPANY_NAME, telefony, emaily,adresy\n",
    "#     filtered_MTCH_PT = MTCH_PT[MTCH_PT['MATCHING_FAMILY_NAME'].isin(MCTH_TMP_PT['MATCHING_FAMILY_NAME'])&\n",
    "#                                ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID'])]\n",
    "#     transformed_df = insertion(filtered_MTCH_PT)\n",
    "\n",
    "#     # Append the transformed data to the MCTH_TMP_PT DataFrame\n",
    "#     MCTH_TMP_PT = MCTH_TMP_PT.append(transformed_df, ignore_index=True)\n",
    "\n",
    "#     print(\"Shape after comparing COMPANY_NAME \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "#     last_count = MCTH_TMP_PT.shape[0]\n",
    "#         filtered_MTCH_PT =  MTCH_PT[\n",
    "#        #MTCH_PT['MATCHING_FAMILY_NAME'].isnull() & \n",
    "#         MTCH_PT['MATCHING_FIRST_NAME'].isin(MCTH_TMP_PT['MATCHING_FIRST_NAME'] &\n",
    "#                                    ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID']))\n",
    "#         ]\n",
    "    filtered_MTCH_PT = MTCH_PT[MTCH_PT['MATCHING_FIRST_NAME'].isin(MCTH_TMP_PT['MATCHING_FIRST_NAME'])&\n",
    "                               ~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID'])]\n",
    "    transformed_df = insertion(filtered_MTCH_PT)\n",
    "\n",
    "    # Append the transformed data to the MCTH_TMP_PT DataFrame\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "    print(\"Shape after comparing FIRST_NAME \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "    \n",
    "    ############### UNIFIED_PARTY_ID #################\n",
    "    filtered_MTCH_PT = MTCH_PT[MTCH_PT['MATCHING_CONSOLIDATED_PARTY_ID'].isin(MCTH_TMP_PT['MATCHING_CONSOLIDATED_PARTY_ID'])\n",
    "                                       & (~MTCH_PT['MATCHING_CONSOLIDATED_PARTY_ID'].isna())\n",
    "                                       & (~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID']))]\n",
    "    transformed_df = insertion(filtered_MTCH_PT)\n",
    "    \n",
    "    # Append the transformed data to the MCTH_TMP_PT DataFrame\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "    print(\"Shape after comparing CONSOLIDATED_PARTY_ID \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "    \n",
    "    MTCH_PT[MTCH_PT['MATCHING_UNIFIED_PARTY_ID'].isin(MCTH_TMP_PT['MATCHING_UNIFIED_PARTY_ID']) & (~MTCH_PT['MATCHING_UNIFIED_PARTY_ID'].isna())]\n",
    "    filtered_MTCH_PT = MTCH_PT[MTCH_PT['MATCHING_UNIFIED_PARTY_ID'].isin(MCTH_TMP_PT['MATCHING_UNIFIED_PARTY_ID'])\n",
    "                               & (~MTCH_PT['MATCHING_UNIFIED_PARTY_ID'].isna())\n",
    "                               & (~MTCH_PT['PARTY_ID'].isin(MCTH_TMP_PT['PARTY_ID']))]\n",
    "    transformed_df = insertion(filtered_MTCH_PT)\n",
    "    MCTH_TMP_PT = pd.concat([MCTH_TMP_PT, transformed_df], ignore_index=True)\n",
    "    print(\"Shape after comparing UNIFIED_PARTY_ID \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "    last_count = MCTH_TMP_PT.shape[0]\n",
    "    \n",
    "    print(\"Shape after comparing CONSOLIDATED_PARTY_ID \",MCTH_TMP_PT.shape[0], f\" - {MCTH_TMP_PT.shape[0]-last_count} Rows Were Added\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    after_adding = MCTH_TMP_PT.shape[0]\n",
    "    i = i+1\n",
    "    num_rows = after_adding - before_adding\n",
    "    print(\"############\")\n",
    "    print(\"\")\n",
    "    print(\"Number of rows added after a cycle\",num_rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREVIOUS_UNIFIED_PARTY_ID NULL COUNT:  11347\n",
      "PREVIOUS_CONSOLIDATED_PARTY_ID NULL COUNT  11347\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attributes filling ...: 11347it [00:02, 4436.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREVIOUS_UNIFIED_PARTY_ID NULL COUNT:  0\n",
      "PREVIOUS_CONSOLIDATED_PARTY_ID NULL COUNT  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PREVIOUS_UNIFIED_PARTY_ID NULL COUNT: \",MCTH_TMP_PT['PREVIOUS_UNIFIED_PARTY_ID'].isna().sum())\n",
    "print(\"PREVIOUS_CONSOLIDATED_PARTY_ID NULL COUNT \",MCTH_TMP_PT['PREVIOUS_CONSOLIDATED_PARTY_ID'].isna().sum())\n",
    "print(\"\")\n",
    "\n",
    "for index, row in tqdm(MCTH_TMP_PT.iterrows(), desc = \"Attributes filling ...\"):\n",
    "    if math.isnan(row['MATCHING_UNIFIED_PARTY_ID']) and  math.isnan(row['MATCHING_CONSOLIDATED_PARTY_ID']):\n",
    "        MCTH_TMP_PT.loc[index, 'PREVIOUS_UNIFIED_PARTY_ID'] = MCTH_TMP_PT.loc[index, 'PARTY_ID']\n",
    "        MCTH_TMP_PT.loc[index, 'PREVIOUS_CONSOLIDATED_PARTY_ID'] = MCTH_TMP_PT.loc[index, 'PARTY_ID']\n",
    "    else:\n",
    "        MCTH_TMP_PT.loc[index, 'PREVIOUS_UNIFIED_PARTY_ID'] = MCTH_TMP_PT.loc[index, 'MATCHING_UNIFIED_PARTY_ID']\n",
    "        MCTH_TMP_PT.loc[index, 'PREVIOUS_CONSOLIDATED_PARTY_ID'] = MCTH_TMP_PT.loc[index, 'MATCHING_CONSOLIDATED_PARTY_ID']\n",
    "print(\"\")\n",
    "print(\"PREVIOUS_UNIFIED_PARTY_ID NULL COUNT: \",MCTH_TMP_PT['PREVIOUS_UNIFIED_PARTY_ID'].isna().sum())\n",
    "print(\"PREVIOUS_CONSOLIDATED_PARTY_ID NULL COUNT \",MCTH_TMP_PT['PREVIOUS_CONSOLIDATED_PARTY_ID'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate groups\n",
    "- MATCHING_PARTY_IDENTIFIER match\n",
    "- FAMILY_NAME match\n",
    "- FIRST_NAME match\n",
    "- EMAIL match\n",
    "- PHONE match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "def is_iterable(obj):\n",
    "    return isinstance(obj, Iterable)\n",
    "\n",
    "def contains_phone_id(phone_id_list, phone_id):\n",
    "    # Check if phone_id_list is not NA and is iterable\n",
    "    #if pd.notna(phone_id_list) and is_iterable(phone_id_list):\n",
    "    if  is_iterable(phone_id_list):\n",
    "        # Iterate over items if it's iterable\n",
    "        return any(phone_id == item for item in phone_id_list if pd.notna(item))\n",
    "    # Handle the case where phone_id_list is a single non-iterable value\n",
    "    # elif pd.notna(phone_id_list):\n",
    "    #     return phone_id == phone_id_list\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def contains_email_id(list_obj, id):\n",
    "    if is_iterable(list_obj):\n",
    "        return any(id == item for item in list_obj if pd.notna(item))\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ...: 100%|██████████| 10726/10726 [00:00<00:00, 785969.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest group is:  (9989288.0,)  with elemts count of  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rozšiřování skupin podle rodinného jména: 100%|██████████| 10726/10726 [00:29<00:00, 362.70it/s]\n",
      "Counting ...: 100%|██████████| 10726/10726 [00:00<00:00, 306815.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest group is:  (9989131.0,)  with elemts count of  167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Přidávání prvků se stejným křestním jménem: 100%|██████████| 10726/10726 [01:31<00:00, 116.70it/s]\n",
      "Counting ...: 100%|██████████| 10726/10726 [00:00<00:00, 935459.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest group is:  (9989131.0,)  with elemts count of  167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Přidávání prvků se stejným ID emailu: 100%|██████████| 10726/10726 [00:36<00:00, 297.63it/s] \n",
      "Counting ...: 100%|██████████| 10726/10726 [00:00<00:00, 338551.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest group is:  (9989131.0,)  with elemts count of  167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Přidávání prvků se stejným ID telefonu: 100%|██████████| 10726/10726 [00:20<00:00, 525.44it/s] \n",
      "Counting ...: 100%|██████████| 10726/10726 [00:00<00:00, 830958.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest group is:  (9989131.0,)  with elemts count of  167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "grouped_by_id = MCTH_TMP_PT.groupby(['MATCHING_PARTY_IDENTIFIER']) #, 'MATCHING_COMPANY_IDENTIFIER' and NOT NULL\n",
    "\n",
    "basic_groups = {id_: group.index.tolist() for id_, group in grouped_by_id}\n",
    "max = 0\n",
    "max_id = 0 \n",
    "for id_, indices in tqdm(basic_groups.items(), desc=\"Counting ...\"):\n",
    "    if len(indices) > max:\n",
    "        max = len(indices)\n",
    "        max_id = id_\n",
    "\n",
    "print(\"The biggest group is: \",max_id, \" with elemts count of \", max)\n",
    "\n",
    "# MATCHING_FAMILY_NAME\n",
    "expanded_groups = {}\n",
    "\n",
    "for id_, indices in tqdm(basic_groups.items(), desc=\"Rozšiřování skupin podle rodinného jména\"):\n",
    "    expanded_group = set(indices)\n",
    "    \n",
    "    for idx in indices:\n",
    "        family_name = MCTH_TMP_PT.loc[idx, 'MATCHING_FAMILY_NAME']\n",
    "        if pd.notna(family_name):\n",
    "            same_family_name_indices = MCTH_TMP_PT.index[MCTH_TMP_PT['MATCHING_FAMILY_NAME'] == family_name].tolist()\n",
    "            expanded_group.update(same_family_name_indices)\n",
    "\n",
    "    expanded_groups[id_] = list(expanded_group)\n",
    "max = 0\n",
    "max_id = 0 \n",
    "for id_, indices in tqdm(expanded_groups.items(), desc=\"Counting ...\"):\n",
    "    if len(indices) > max:\n",
    "        max = len(indices)\n",
    "        max_id = id_\n",
    "\n",
    "print(\"The biggest group is: \",max_id, \" with elemts count of \", max)\n",
    "\n",
    "# MATCHING_FIRST_NAME\n",
    "expanded_groups_first_name = {}\n",
    "\n",
    "for id_, indices in tqdm(expanded_groups.items(), desc=\"Přidávání prvků se stejným křestním jménem\"):\n",
    "    group_first_name = set(indices)\n",
    "\n",
    "    for idx in indices:\n",
    "        first_name = MCTH_TMP_PT.loc[idx, 'MATCHING_FIRST_NAME']\n",
    "        if pd.notna(first_name):\n",
    "            same_first_name_indices = MCTH_TMP_PT.index[(MCTH_TMP_PT['MATCHING_FAMILY_NAME'].isnull()) & (MCTH_TMP_PT['MATCHING_FIRST_NAME'] == first_name)].tolist()\n",
    "            group_first_name.update(same_first_name_indices)\n",
    "\n",
    "    expanded_groups_first_name[id_] = list(group_first_name)\n",
    "\n",
    "max = 0\n",
    "max_id = 0 \n",
    "for id_, indices in tqdm(expanded_groups_first_name.items(), desc=\"Counting ...\"):\n",
    "    if len(indices) > max:\n",
    "        max = len(indices)\n",
    "        max_id = id_\n",
    "\n",
    "print(\"The biggest group is: \",max_id, \" with elemts count of \", max)\n",
    "#TODO: EMAILS \n",
    "expanded_groups_email = {}\n",
    "for id_, indices in tqdm(expanded_groups_first_name.items(), desc=\"Přidávání prvků se stejným ID emailu\"):\n",
    "    expanded_groups =set(indices)\n",
    "    for idx in indices:\n",
    "        idx_subset = MCTH_TMP_PT.loc[idx, 'UNIFICATION_PARTY_EMAIL_ID']\n",
    "\n",
    "        if isinstance(idx_subset, (list, set, tuple)):\n",
    "            # Pokud je idx_subset seznam, set nebo tuple\n",
    "            if not idx_subset or all(pd.isna(element) for element in idx_subset):\n",
    "                continue\n",
    "            email_id_list = set(idx_subset)\n",
    "        else:\n",
    "            # Pokud je idx_subset jedna hodnota (není seznam, set, nebo tuple)\n",
    "            if pd.isna(idx_subset):\n",
    "                continue\n",
    "            email_id_list = {idx_subset}\n",
    "\n",
    "        for email_id in email_id_list:\n",
    "            if email_id:\n",
    "                same_email_id_indices = MCTH_TMP_PT.index[MCTH_TMP_PT['UNIFICATION_PARTY_EMAIL_ID'].apply(lambda x: contains_email_id(x, email_id))].tolist()\n",
    "                expanded_groups.update(same_email_id_indices)\n",
    "      \n",
    "    expanded_groups_email[id_] = list(expanded_groups)\n",
    "\n",
    "max = 0\n",
    "max_id = 0 \n",
    "for id_, indices in tqdm(expanded_groups_email.items(), desc=\"Counting ...\"):\n",
    "    if len(indices) > max:\n",
    "        max = len(indices)\n",
    "        max_id = id_\n",
    "\n",
    "print(\"The biggest group is: \",max_id, \" with elemts count of \", max)\n",
    "#TODO: phones \n",
    "final_groups = {}\n",
    "for id_, indices in tqdm(expanded_groups_email.items(), desc=\"Přidávání prvků se stejným ID telefonu\"):\n",
    "    final_group = set(indices)\n",
    "    for idx in indices:\n",
    "        idx_subset = MCTH_TMP_PT.loc[idx, 'UNIFICATION_PARTY_PHONE_ID']\n",
    "        if isinstance(idx_subset, (list, set, tuple)):        \n",
    "            if not idx_subset or all(pd.isna(element) for element in idx_subset):\n",
    "                continue\n",
    "            phone_id_list = set(idx_subset)\n",
    "        else:\n",
    "            if pd.isna(idx_subset):\n",
    "                    continue\n",
    "            phone_id_list = {idx_subset}\n",
    "        \n",
    "        for phone_id in phone_id_list:\n",
    "            if phone_id:\n",
    "                same_phone_id_indices = MCTH_TMP_PT.index[MCTH_TMP_PT['UNIFICATION_PARTY_PHONE_ID'].apply(lambda x: contains_phone_id(x, phone_id))].tolist()\n",
    "                final_group.update(same_phone_id_indices)\n",
    "    final_groups[id_] = list(final_group)\n",
    "max = 0\n",
    "max_id = 0 \n",
    "for id_, indices in tqdm(final_groups.items(), desc=\"Counting ...\"):\n",
    "    if len(indices) > max:\n",
    "        max = len(indices)\n",
    "        max_id = id_\n",
    "\n",
    "print(\"The biggest group is: \",max_id, \" with elemts count of \", max)\n",
    "\n",
    "\n",
    "group_id_to_indices = {group_id: indices for group_id, indices in enumerate(final_groups.values())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impaired data filling \n",
    " - MATCHING_CONSOLIDATED_PARTY_ID\n",
    " - MATCHING_UNIFIED_PARTY_ID\n",
    " - L1_MASTER_PARTY_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zpracování hodnot bez par: 100%|██████████| 10726/10726 [00:04<00:00, 2369.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8913  elements were changed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "elements_changed = 0 \n",
    "for key, value in tqdm(group_id_to_indices.items(), desc=\"Zpracování hodnot bez par\"):\n",
    "    if len(value) == 1:\n",
    "        elements_changed += 1\n",
    "        MCTH_TMP_PT.loc[value[0],'MATCHING_CONSOLIDATED_PARTY_ID'] = MCTH_TMP_PT.loc[value[0]]['PREVIOUS_CONSOLIDATED_PARTY_ID']\n",
    "        MCTH_TMP_PT.loc[value[0],'MATCHING_UNIFIED_PARTY_ID'] = MCTH_TMP_PT.loc[value[0]]['PREVIOUS_UNIFIED_PARTY_ID']\n",
    "        MCTH_TMP_PT.loc[value[0],'L1_MASTER_PARTY_ID'] = MCTH_TMP_PT.loc[value[0]]['PARTY_ID']\n",
    "print(elements_changed,\" elements were changed \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired data NULL filling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paired data NULL filling: 100%|██████████| 10726/10726 [00:12<00:00, 855.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, value in tqdm(group_id_to_indices.items(), desc=\"Paired data NULL filling\"):\n",
    "    if len(value)  > 1:\n",
    "        for v in value:\n",
    "            elements_changed += 1\n",
    "\n",
    "            MCTH_TMP_PT.loc[v,'MATCHING_CONSOLIDATED_PARTY_ID'] = MCTH_TMP_PT.loc[v]['PREVIOUS_CONSOLIDATED_PARTY_ID']\n",
    "            MCTH_TMP_PT.loc[v,'MATCHING_UNIFIED_PARTY_ID'] = MCTH_TMP_PT.loc[v]['PREVIOUS_UNIFIED_PARTY_ID']\n",
    "            MCTH_TMP_PT.loc[v,'L1_MASTER_PARTY_ID'] = MCTH_TMP_PT.loc[v]['PARTY_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client groups NAME_SURNAME_BIRTH_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Definice funkce pro výpočet skóre podobnosti\n",
    "def similarity_score(row1, row2):    \n",
    "    \"\"\" Function defenition to calulate similiarity score: NAME_SURNAME_BIRTH_DATE\"\"\"\n",
    "    \n",
    "    name_score = fuzz.partial_ratio(f\"{row1['MATCHING_FIRST_NAME']} {row1['MATCHING_FAMILY_NAME']}\", \n",
    "                                    f\"{row2['MATCHING_FIRST_NAME']} {row2['MATCHING_FAMILY_NAME']}\") #  TODO: + PARTY_TYPE_ID\n",
    "    birth_date_score = 100 if row1['MATCHING_BIRTH_DATE'] == row2['MATCHING_BIRTH_DATE'] else 0\n",
    "\n",
    "    return (name_score + birth_date_score) / 2 \n",
    "\n",
    "def similarity_score_list_column(row1, row2, column_name):\n",
    "    \"\"\" Function defenition to calulate similiarity score for columns with list\"\"\"\n",
    "    \n",
    "    name_score = fuzz.partial_ratio(f\"{row1['MATCHING_FIRST_NAME']} {row1['MATCHING_FAMILY_NAME']}\", \n",
    "                                    f\"{row2['MATCHING_FIRST_NAME']} {row2['MATCHING_FAMILY_NAME']}\")\n",
    "    #print(row1[column_name])\n",
    "    #print(row2[column_name])\n",
    "    if (isinstance(row1[column_name], (list, set, tuple)) and \n",
    "        isinstance(row2[column_name], (list, set, tuple))):\n",
    "        row1_set = set(row1[column_name])\n",
    "        row2_set = set(row2[column_name])\n",
    "        if row1_set.intersection(row2_set):\n",
    "            second_score = 100\n",
    "        else:\n",
    "            second_score = 0\n",
    "        return (name_score + second_score) / 2\n",
    "    else:\n",
    "        return 0 \n",
    "            \n",
    "    # if pd.isna(row1[column_name]) or pd.isna(row2[column_name]):\n",
    "    #     return 0 \n",
    "    # else:\n",
    "    #     row1_set = set(row1[column_name])\n",
    "    #     row2_set = set(row2[column_name])\n",
    "    #     if row1_set.intersection(row2_set):\n",
    "    #         second_score = 100\n",
    "    #     else:\n",
    "    #         second_score = 0\n",
    "    #     return (name_score + second_score) / 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zpracování primárních skupin: 100%|██████████| 10726/10726 [08:37<00:00, 20.74it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Vytvoření skupin klientů\n",
    "client_groups = {}\n",
    "#client_groups_low_similarity = {}\n",
    "\n",
    "\n",
    "for group_id, indices in tqdm(group_id_to_indices.items(), desc=\"Zpracování primárních skupin\"):\n",
    "\n",
    "    if len(indices) > 1:\n",
    "        current_group = MCTH_TMP_PT.loc[indices]\n",
    "        client_group = []\n",
    "        #client_group_low_similarity = []\n",
    "        threshold = 90\n",
    "        for i in range(len(current_group)):\n",
    "            for j in range(i + 1, len(current_group)):\n",
    "                if similarity_score(current_group.iloc[i], current_group.iloc[j]) >= threshold or similarity_score_list_column(current_group.iloc[i], current_group.iloc[j], \"UNIFICATION_PARTY_EMAIL_ID\") >= threshold or similarity_score_list_column(current_group.iloc[i], current_group.iloc[j], \"UNIFICATION_PARTY_PHONE_ID\") >= threshold:\n",
    "                    client_group.append((current_group.index[i], current_group.index[j]))\n",
    "                # else:\n",
    "                #     client_group_low_similarity.append((current_group.index[i], current_group.index[j]))\n",
    "        #client_groups_low_similarity[group_id] = client_group_low_similarity\n",
    "        client_groups[group_id] = client_group\n",
    "    # else:\n",
    "    #     client_groups[group_id] = [indices[0]]\n",
    "\n",
    "# Výpis výsledků\n",
    "# for group_id, pairs in client_groups.items():\n",
    "#     if pairs:\n",
    "#         print(f\"ID skupiny: {group_id}, Páry klientů: {pairs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal key search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client_to_group = {}\n",
    "client_to_group_unified = {}\n",
    "\n",
    "for group_id, client_pairs in client_groups.items():\n",
    "    ind_list = []\n",
    "    ind_list_PO = []\n",
    "    ind_list_FO = []\n",
    "    ind_list_FOP = []\n",
    "    if len(client_pairs) > 1:\n",
    "        for client_pairs in client_groups[group_id]:\n",
    "            for pair in client_pairs:\n",
    "                \n",
    "                if (MCTH_TMP_PT.loc[pair, 'MATCHING_PARTY_TYPE_ID'] == 'FO'):\n",
    "                    ind_list_FO.append(MCTH_TMP_PT.loc[pair, 'PARTY_ID'])\n",
    "                    ind_list.append(MCTH_TMP_PT.loc[pair, 'PARTY_ID'])\n",
    "                if (MCTH_TMP_PT.loc[pair, 'MATCHING_PARTY_TYPE_ID'] == 'PO'):\n",
    "                    ind_list_PO.append(MCTH_TMP_PT.loc[pair, 'PARTY_ID'])               \n",
    "                if (MCTH_TMP_PT.loc[pair, 'MATCHING_PARTY_TYPE_ID'] == 'FOP'):\n",
    "                    ind_list_FOP.append(MCTH_TMP_PT.loc[pair, 'PARTY_ID']) \n",
    "                    ind_list.append(MCTH_TMP_PT.loc[pair, 'PARTY_ID']) # ???              \n",
    "        \n",
    "            if len(ind_list) > 0:\n",
    "                group_id = min(ind_list)\n",
    "                #client_to_group[pair] = group_id\n",
    "            # print(group_id)\n",
    "            if len(ind_list_FO) > 0:            \n",
    "                group_id_FO = min(ind_list_FO)\n",
    "                #client_to_group_PO[ind_list_PO] = group_id_PO\n",
    "            # print(group_id_PO)\n",
    "            \n",
    "            if len(ind_list_PO) > 0:\n",
    "                group_id_PO = min(ind_list_PO)\n",
    "            if len(ind_list_FOP) > 0:\n",
    "                group_id_FOP = min(ind_list_FOP)\n",
    "            # client_to_group_FO[ind_list_FO] = group_id_FO\n",
    "            # print(group_id_FO)   \n",
    "            \n",
    "            client_to_group[pair] = group_id                     \n",
    "            if (MCTH_TMP_PT.loc[pair, 'MATCHING_PARTY_TYPE_ID'] == 'FO'): #| (MCTH_TMP_PT.loc[pair, 'MATCHING_PERSON_TYPE_ID']=='FOP'):\n",
    "                client_to_group_unified[pair] = group_id_FO\n",
    "            if (MCTH_TMP_PT.loc[pair, 'MATCHING_PARTY_TYPE_ID'] == 'PO'): #| (MCTH_TMP_PT.loc[pair, 'MATCHING_PERSON_TYPE_ID']=='FOP'):\n",
    "                client_to_group_unified[pair] = group_id_PO\n",
    "            if (MCTH_TMP_PT.loc[pair, 'MATCHING_PARTY_TYPE_ID'] == 'FOP'): #| \n",
    "                client_to_group_unified[pair] = group_id_FOP\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding CONSOLIDATED, UNIFIYED ID, and ROLE to MCTH_TMP_PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zpracování pravidel: 100%|██████████| 1813/1813 [00:00<00:00, 13809.73it/s]\n",
      "Zpracování hodnot bez par: 100%|██████████| 477/477 [00:00<00:00, 11082.70it/s]\n",
      "Zpracování hodnot bez par: 100%|██████████| 477/477 [00:00<00:00, 4419.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(MCTH_TMP_PT['MATCHING_UNIFIED_PARTY_ID'].isna().sum())\n",
    "print(MCTH_TMP_PT['PREVIOUS_CONSOLIDATED_PARTY_ID'].isna().sum())\n",
    "print(MCTH_TMP_PT['MATCHING_CONSOLIDATED_PARTY_ID'].isna().sum())\n",
    "print(MCTH_TMP_PT['PREVIOUS_UNIFIED_PARTY_ID'].isna().sum())\n",
    "print(\"\")\n",
    "\n",
    "for  key, value in tqdm(client_groups.items(), desc=\"Zpracování pravidel\"):\n",
    "    MCTH_TMP_PT.loc[key,'L1_MASTER_MATCHING_RULE'] = \"NAME_SURNAME\"\n",
    "\n",
    "\n",
    "for key, value in tqdm(client_to_group.items(), desc=\"Zpracování hodnot bez par\"):\n",
    "    MCTH_TMP_PT.loc[key,'MATCHING_CONSOLIDATED_PARTY_ID'] = value \n",
    "\n",
    "for key, value in tqdm(client_to_group_unified.items(), desc=\"Zpracování hodnot bez par\"):\n",
    "    if MCTH_TMP_PT.loc[key,'MATCHING_PARTY_TYPE_ID'] == 'PO':\n",
    "        MCTH_TMP_PT.loc[key,'MATCHING_CONSOLIDATED_PARTY_ID'] = value \n",
    "    MCTH_TMP_PT.loc[key,'MATCHING_UNIFIED_PARTY_ID'] = value\n",
    "    MCTH_TMP_PT.loc[key,'L1_MASTER_PARTY_ID'] = value\n",
    "\n",
    "for index, row in MCTH_TMP_PT.iterrows():\n",
    "    if row['PARTY_ID'] == row['MATCHING_UNIFIED_PARTY_ID']:\n",
    "        MCTH_TMP_PT.loc[index,'L1_RECORD_ROLE'] = 'MASTER'        \n",
    "    else:\n",
    "        MCTH_TMP_PT.loc[index,'L1_RECORD_ROLE'] = 'SLAVE'\n",
    "\n",
    "print(MCTH_TMP_PT['MATCHING_UNIFIED_PARTY_ID'].isna().sum())\n",
    "print(MCTH_TMP_PT['PREVIOUS_CONSOLIDATED_PARTY_ID'].isna().sum())\n",
    "print(MCTH_TMP_PT['MATCHING_CONSOLIDATED_PARTY_ID'].isna().sum())\n",
    "print(MCTH_TMP_PT['PREVIOUS_UNIFIED_PARTY_ID'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
